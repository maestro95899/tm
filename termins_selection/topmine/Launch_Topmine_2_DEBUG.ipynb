{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = open(\"vw_remont-i-stroitel_stvo_only_text_grouped_by_worker\", \"r\")\n",
    "sample = open('docs_sample_1000', 'w')\n",
    "i = 0\n",
    "for line in full:\n",
    "    if i < 1000:\n",
    "        sample.write(line)\n",
    "    i += 1\n",
    "sample.close()\n",
    "full.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Phrase Mining...\n",
      "Start Mine10\n",
      "Start _frequentPatternMining\n",
      "_frequentPatternMining: len(documents) = 1000\n",
      "circle 2 len(documents) = 1000\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 3\n",
      "circle 3 len(documents) = 999\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 4\n",
      "circle 4 len(documents) = 927\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 5\n",
      "circle 5 len(documents) = 662\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "Process _frequentPatternMining: n = 6\n",
      "circle 6 len(documents) = 339\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "Process _frequentPatternMining: n = 7\n",
      "circle 7 len(documents) = 185\n",
      "doc num 0\n",
      "doc num 100\n",
      "Process _frequentPatternMining: n = 8\n",
      "circle 8 len(documents) = 56\n",
      "doc num 0\n",
      "Process _frequentPatternMining: n = 9\n",
      "circle 9 len(documents) = 35\n",
      "doc num 0\n",
      "Process _frequentPatternMining: n = 10\n",
      "0\n",
      "монтаж 2829\n",
      "\n",
      "1\n",
      "душевой 72\n",
      "\n",
      "2\n",
      "кабинки 31\n",
      "\n",
      "3\n",
      "демонтаж 1060\n",
      "\n",
      "4\n",
      "счетчиков 39\n",
      "\n",
      "5\n",
      "воды 195\n",
      "\n",
      "6\n",
      "и 5904\n",
      "\n",
      "7\n",
      "тепла 27\n",
      "\n",
      "8\n",
      "прокладывание 23\n",
      "\n",
      "9\n",
      "труб 245\n",
      "\n",
      "10\n",
      "отопления 285\n",
      "\n",
      "len(doc_phrases) = 1000\n"
     ]
    }
   ],
   "source": [
    "import topmine_src\n",
    "import importlib # использую для перегрузки библиотек\n",
    "import topmine_src.phrase_mining as phrase_mining\n",
    "import sys\n",
    "import topmine_src.utils as utils\n",
    "\n",
    "importlib.reload(topmine_src.phrase_mining)\n",
    "importlib.reload(topmine_src.utils)\n",
    "\n",
    "arguments = sys.argv\n",
    "print('Running Phrase Mining...')\n",
    "\n",
    "file_name = \"docs_sample_1000\"\n",
    "output_path = \"DEBUG_remont_n-grams_10_4\"\n",
    "\n",
    "# represents the minimum number of occurences you want each phrase to have.\n",
    "min_support=10 \n",
    "\n",
    "# represents the threshold for merging two words into a phrase. A lower value\n",
    "# alpha leads to higher recall and lower precision,\n",
    "alpha=4\n",
    "\n",
    "# length of the maximum phrase size\n",
    "max_phrase_size=10\n",
    "\n",
    "phrase_miner = phrase_mining.PhraseMining(file_name, min_support, max_phrase_size, alpha);\n",
    "partitioned_docs, index_vocab = phrase_miner.mine()\n",
    "frequent_phrases = phrase_miner.get_frequent_phrases(min_support)\n",
    "frequent_hash_phrases = phrase_miner.get_frequent_phrases(min_support, counter_type=\"hash\")\n",
    "utils.store_partitioned_docs(partitioned_docs, output_path)\n",
    "utils.store_vocab(index_vocab, output_path)\n",
    "utils.store_frequent_phrases(frequent_phrases, output_path)\n",
    "utils.store_frequent_phrases(frequent_hash_phrases, output_path + \"_hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Phrase Mining...\n",
      "Start Mine10\n",
      "Start _frequentPatternMining\n",
      "_frequentPatternMining: len(documents) = 1000\n",
      "circle 2 len(documents) = 1000\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 3\n",
      "circle 3 len(documents) = 999\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 4\n",
      "circle 4 len(documents) = 927\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 5\n",
      "circle 5 len(documents) = 662\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "Process _frequentPatternMining: n = 6\n",
      "circle 6 len(documents) = 339\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "Process _frequentPatternMining: n = 7\n",
      "circle 7 len(documents) = 185\n",
      "doc num 0\n",
      "doc num 100\n",
      "Process _frequentPatternMining: n = 8\n",
      "circle 8 len(documents) = 56\n",
      "doc num 0\n",
      "Process _frequentPatternMining: n = 9\n",
      "circle 9 len(documents) = 35\n",
      "doc num 0\n",
      "Process _frequentPatternMining: n = 10\n",
      "0\n",
      "монтаж 2829\n",
      "\n",
      "1\n",
      "душевой 72\n",
      "\n",
      "2\n",
      "кабинки 31\n",
      "\n",
      "3\n",
      "демонтаж 1060\n",
      "\n",
      "4\n",
      "счетчиков 39\n",
      "\n",
      "5\n",
      "воды 195\n",
      "\n",
      "6\n",
      "и 5904\n",
      "\n",
      "7\n",
      "тепла 27\n",
      "\n",
      "8\n",
      "прокладывание 23\n",
      "\n",
      "9\n",
      "труб 245\n",
      "\n",
      "10\n",
      "отопления 285\n",
      "\n",
      "len(doc_phrases) = 1000\n"
     ]
    }
   ],
   "source": [
    "import topmine_src\n",
    "import importlib # использую для перегрузки библиотек\n",
    "import topmine_src.phrase_mining as phrase_mining\n",
    "import sys\n",
    "import topmine_src.utils as utils\n",
    "\n",
    "importlib.reload(topmine_src.phrase_mining)\n",
    "importlib.reload(topmine_src.utils)\n",
    "\n",
    "arguments = sys.argv\n",
    "print('Running Phrase Mining...')\n",
    "\n",
    "file_name = \"docs_sample_1000\"\n",
    "output_path = \"DEBUG_remont_n-grams_10_1\"\n",
    "\n",
    "# represents the minimum number of occurences you want each phrase to have.\n",
    "min_support=10 \n",
    "\n",
    "# represents the threshold for merging two words into a phrase. A lower value\n",
    "# alpha leads to higher recall and lower precision,\n",
    "alpha=1\n",
    "\n",
    "# length of the maximum phrase size\n",
    "max_phrase_size=10\n",
    "\n",
    "phrase_miner = phrase_mining.PhraseMining(file_name, min_support, max_phrase_size, alpha);\n",
    "partitioned_docs, index_vocab = phrase_miner.mine()\n",
    "frequent_phrases = phrase_miner.get_frequent_phrases(min_support)\n",
    "frequent_hash_phrases = phrase_miner.get_frequent_phrases(min_support, counter_type=\"hash\")\n",
    "utils.store_partitioned_docs(partitioned_docs, output_path)\n",
    "utils.store_vocab(index_vocab, output_path)\n",
    "utils.store_frequent_phrases(frequent_phrases, output_path)\n",
    "utils.store_frequent_phrases(frequent_hash_phrases, output_path + \"_hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Phrase Mining...\n",
      "Start Mine10\n",
      "Start _frequentPatternMining\n",
      "_frequentPatternMining: len(documents) = 1000\n",
      "circle 2 len(documents) = 1000\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 3\n",
      "circle 3 len(documents) = 999\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 4\n",
      "circle 4 len(documents) = 927\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 5\n",
      "circle 5 len(documents) = 662\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "Process _frequentPatternMining: n = 6\n",
      "circle 6 len(documents) = 339\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "Process _frequentPatternMining: n = 7\n",
      "circle 7 len(documents) = 185\n",
      "doc num 0\n",
      "doc num 100\n",
      "Process _frequentPatternMining: n = 8\n",
      "circle 8 len(documents) = 56\n",
      "doc num 0\n",
      "Process _frequentPatternMining: n = 9\n",
      "circle 9 len(documents) = 35\n",
      "doc num 0\n",
      "Process _frequentPatternMining: n = 10\n",
      "0\n",
      "монтаж 2829\n",
      "\n",
      "1\n",
      "душевой 72\n",
      "\n",
      "2\n",
      "кабинки 31\n",
      "\n",
      "3\n",
      "демонтаж 1060\n",
      "\n",
      "4\n",
      "счетчиков 39\n",
      "\n",
      "5\n",
      "воды 195\n",
      "\n",
      "6\n",
      "и 5904\n",
      "\n",
      "7\n",
      "тепла 27\n",
      "\n",
      "8\n",
      "прокладывание 23\n",
      "\n",
      "9\n",
      "труб 245\n",
      "\n",
      "10\n",
      "отопления 285\n",
      "\n",
      "len(doc_phrases) = 1000\n"
     ]
    }
   ],
   "source": [
    "import topmine_src\n",
    "import importlib # использую для перегрузки библиотек\n",
    "import topmine_src.phrase_mining as phrase_mining\n",
    "import sys\n",
    "import topmine_src.utils as utils\n",
    "\n",
    "importlib.reload(topmine_src.phrase_mining)\n",
    "importlib.reload(topmine_src.utils)\n",
    "\n",
    "arguments = sys.argv\n",
    "print('Running Phrase Mining...')\n",
    "\n",
    "file_name = \"docs_sample_1000\"\n",
    "output_path = \"DEBUG_remont_n-grams_10_20\"\n",
    "\n",
    "# represents the minimum number of occurences you want each phrase to have.\n",
    "min_support=10 \n",
    "\n",
    "# represents the threshold for merging two words into a phrase. A lower value\n",
    "# alpha leads to higher recall and lower precision,\n",
    "alpha=20\n",
    "\n",
    "# length of the maximum phrase size\n",
    "max_phrase_size=10\n",
    "\n",
    "phrase_miner = phrase_mining.PhraseMining(file_name, min_support, max_phrase_size, alpha);\n",
    "partitioned_docs, index_vocab = phrase_miner.mine()\n",
    "frequent_phrases = phrase_miner.get_frequent_phrases(min_support)\n",
    "frequent_hash_phrases = phrase_miner.get_frequent_phrases(min_support, counter_type=\"hash\")\n",
    "utils.store_partitioned_docs(partitioned_docs, output_path)\n",
    "utils.store_vocab(index_vocab, output_path)\n",
    "utils.store_frequent_phrases(frequent_phrases, output_path)\n",
    "utils.store_frequent_phrases(frequent_hash_phrases, output_path + \"_hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Phrase Mining...\n",
      "Start Mine10\n",
      "Start _frequentPatternMining\n",
      "_frequentPatternMining: len(documents) = 1000\n",
      "circle 2 len(documents) = 1000\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 3\n",
      "circle 3 len(documents) = 999\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 4\n",
      "circle 4 len(documents) = 927\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 5\n",
      "circle 5 len(documents) = 662\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "Process _frequentPatternMining: n = 6\n",
      "circle 6 len(documents) = 339\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "Process _frequentPatternMining: n = 7\n",
      "circle 7 len(documents) = 185\n",
      "doc num 0\n",
      "doc num 100\n",
      "Process _frequentPatternMining: n = 8\n",
      "circle 8 len(documents) = 56\n",
      "doc num 0\n",
      "Process _frequentPatternMining: n = 9\n",
      "circle 9 len(documents) = 35\n",
      "doc num 0\n",
      "Process _frequentPatternMining: n = 10\n",
      "0\n",
      "монтаж 2829\n",
      "\n",
      "1\n",
      "душевой 72\n",
      "\n",
      "2\n",
      "кабинки 31\n",
      "\n",
      "3\n",
      "демонтаж 1060\n",
      "\n",
      "4\n",
      "счетчиков 39\n",
      "\n",
      "5\n",
      "воды 195\n",
      "\n",
      "6\n",
      "и 5904\n",
      "\n",
      "7\n",
      "тепла 27\n",
      "\n",
      "8\n",
      "прокладывание 23\n",
      "\n",
      "9\n",
      "труб 245\n",
      "\n",
      "10\n",
      "отопления 285\n",
      "\n",
      "len(doc_phrases) = 1000\n"
     ]
    }
   ],
   "source": [
    "import topmine_src\n",
    "import importlib # использую для перегрузки библиотек\n",
    "import topmine_src.phrase_mining as phrase_mining\n",
    "import sys\n",
    "import topmine_src.utils as utils\n",
    "\n",
    "importlib.reload(topmine_src.phrase_mining)\n",
    "importlib.reload(topmine_src.utils)\n",
    "\n",
    "arguments = sys.argv\n",
    "print('Running Phrase Mining...')\n",
    "\n",
    "file_name = \"docs_sample_1000\"\n",
    "output_path = \"DEBUG_remont_n-grams_10_200\"\n",
    "\n",
    "# represents the minimum number of occurences you want each phrase to have.\n",
    "min_support=10 \n",
    "\n",
    "# represents the threshold for merging two words into a phrase. A lower value\n",
    "# alpha leads to higher recall and lower precision,\n",
    "alpha=200\n",
    "\n",
    "# length of the maximum phrase size\n",
    "max_phrase_size=10\n",
    "\n",
    "phrase_miner = phrase_mining.PhraseMining(file_name, min_support, max_phrase_size, alpha);\n",
    "partitioned_docs, index_vocab = phrase_miner.mine()\n",
    "frequent_phrases = phrase_miner.get_frequent_phrases(min_support)\n",
    "frequent_hash_phrases = phrase_miner.get_frequent_phrases(min_support, counter_type=\"hash\")\n",
    "utils.store_partitioned_docs(partitioned_docs, output_path)\n",
    "utils.store_vocab(index_vocab, output_path)\n",
    "utils.store_frequent_phrases(frequent_phrases, output_path)\n",
    "utils.store_frequent_phrases(frequent_hash_phrases, output_path + \"_hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Phrase Mining...\n",
      "Start Mine10\n",
      "Start _frequentPatternMining\n",
      "_frequentPatternMining: len(documents) = 1000\n",
      "circle 2 len(documents) = 1000\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 3\n",
      "circle 3 len(documents) = 999\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 4\n",
      "circle 4 len(documents) = 927\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 5\n",
      "circle 5 len(documents) = 662\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "Process _frequentPatternMining: n = 6\n",
      "circle 6 len(documents) = 339\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "Process _frequentPatternMining: n = 7\n",
      "circle 7 len(documents) = 185\n",
      "doc num 0\n",
      "doc num 100\n",
      "Process _frequentPatternMining: n = 8\n",
      "circle 8 len(documents) = 56\n",
      "doc num 0\n",
      "Process _frequentPatternMining: n = 9\n",
      "circle 9 len(documents) = 35\n",
      "doc num 0\n",
      "Process _frequentPatternMining: n = 10\n",
      "0\n",
      "монтаж 2829\n",
      "\n",
      "1\n",
      "душевой 72\n",
      "\n",
      "2\n",
      "кабинки 31\n",
      "\n",
      "3\n",
      "демонтаж 1060\n",
      "\n",
      "4\n",
      "счетчиков 39\n",
      "\n",
      "5\n",
      "воды 195\n",
      "\n",
      "6\n",
      "и 5904\n",
      "\n",
      "7\n",
      "тепла 27\n",
      "\n",
      "8\n",
      "прокладывание 23\n",
      "\n",
      "9\n",
      "труб 245\n",
      "\n",
      "10\n",
      "отопления 285\n",
      "\n",
      "len(doc_phrases) = 1000\n"
     ]
    }
   ],
   "source": [
    "import topmine_src\n",
    "import importlib # использую для перегрузки библиотек\n",
    "import topmine_src.phrase_mining as phrase_mining\n",
    "import sys\n",
    "import topmine_src.utils as utils\n",
    "\n",
    "importlib.reload(topmine_src.phrase_mining)\n",
    "importlib.reload(topmine_src.utils)\n",
    "\n",
    "arguments = sys.argv\n",
    "print('Running Phrase Mining...')\n",
    "\n",
    "file_name = \"docs_sample_1000\"\n",
    "output_path = \"DEBUG_remont_n-grams_10_1000\"\n",
    "\n",
    "# represents the minimum number of occurences you want each phrase to have.\n",
    "min_support=10 \n",
    "\n",
    "# represents the threshold for merging two words into a phrase. A lower value\n",
    "# alpha leads to higher recall and lower precision,\n",
    "alpha=1000\n",
    "\n",
    "# length of the maximum phrase size\n",
    "max_phrase_size=10\n",
    "\n",
    "phrase_miner = phrase_mining.PhraseMining(file_name, min_support, max_phrase_size, alpha);\n",
    "partitioned_docs, index_vocab = phrase_miner.mine()\n",
    "frequent_phrases = phrase_miner.get_frequent_phrases(min_support)\n",
    "frequent_hash_phrases = phrase_miner.get_frequent_phrases(min_support, counter_type=\"hash\")\n",
    "utils.store_partitioned_docs(partitioned_docs, output_path)\n",
    "utils.store_vocab(index_vocab, output_path)\n",
    "utils.store_frequent_phrases(frequent_phrases, output_path)\n",
    "utils.store_frequent_phrases(frequent_hash_phrases, output_path + \"_hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Phrase Mining...\n",
      "Start Mine10\n",
      "Start _frequentPatternMining\n",
      "_frequentPatternMining: len(documents) = 1000\n",
      "circle 2 len(documents) = 1000\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 3\n",
      "circle 3 len(documents) = 1000\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 4\n",
      "circle 4 len(documents) = 979\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 5\n",
      "circle 5 len(documents) = 899\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "Process _frequentPatternMining: n = 6\n",
      "circle 6 len(documents) = 688\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "Process _frequentPatternMining: n = 7\n",
      "circle 7 len(documents) = 444\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "Process _frequentPatternMining: n = 8\n",
      "circle 8 len(documents) = 274\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "Process _frequentPatternMining: n = 9\n",
      "circle 9 len(documents) = 161\n",
      "doc num 0\n",
      "doc num 100\n",
      "Process _frequentPatternMining: n = 10\n",
      "0\n",
      "монтаж 2829\n",
      "\n",
      "1\n",
      "душевой 72\n",
      "\n",
      "2\n",
      "кабинки 31\n",
      "\n",
      "3\n",
      "демонтаж 1060\n",
      "\n",
      "4\n",
      "счетчиков 39\n",
      "\n",
      "5\n",
      "воды 195\n",
      "\n",
      "6\n",
      "и 5904\n",
      "\n",
      "7\n",
      "тепла 27\n",
      "\n",
      "8\n",
      "прокладывание 23\n",
      "\n",
      "9\n",
      "труб 245\n",
      "\n",
      "10\n",
      "отопления 285\n",
      "\n",
      "len(doc_phrases) = 1000\n"
     ]
    }
   ],
   "source": [
    "import topmine_src\n",
    "import importlib # использую для перегрузки библиотек\n",
    "import topmine_src.phrase_mining as phrase_mining\n",
    "import sys\n",
    "import topmine_src.utils as utils\n",
    "\n",
    "importlib.reload(topmine_src.phrase_mining)\n",
    "importlib.reload(topmine_src.utils)\n",
    "\n",
    "arguments = sys.argv\n",
    "print('Running Phrase Mining...')\n",
    "\n",
    "file_name = \"docs_sample_1000\"\n",
    "output_path = \"DEBUG_remont_n-grams_2_4\"\n",
    "\n",
    "# represents the minimum number of occurences you want each phrase to have.\n",
    "min_support=2 \n",
    "\n",
    "# represents the threshold for merging two words into a phrase. A lower value\n",
    "# alpha leads to higher recall and lower precision,\n",
    "alpha=4\n",
    "\n",
    "# length of the maximum phrase size\n",
    "max_phrase_size=10\n",
    "\n",
    "phrase_miner = phrase_mining.PhraseMining(file_name, min_support, max_phrase_size, alpha);\n",
    "partitioned_docs, index_vocab = phrase_miner.mine()\n",
    "frequent_phrases = phrase_miner.get_frequent_phrases(min_support)\n",
    "frequent_hash_phrases = phrase_miner.get_frequent_phrases(min_support, counter_type=\"hash\")\n",
    "utils.store_partitioned_docs(partitioned_docs, output_path)\n",
    "utils.store_vocab(index_vocab, output_path)\n",
    "utils.store_frequent_phrases(frequent_phrases, output_path)\n",
    "utils.store_frequent_phrases(frequent_hash_phrases, output_path + \"_hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Phrase Mining...\n",
      "Start Mine10\n",
      "Start _frequentPatternMining\n",
      "_frequentPatternMining: len(documents) = 1000\n",
      "circle 2 len(documents) = 1000\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 3\n",
      "circle 3 len(documents) = 1000\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 4\n",
      "circle 4 len(documents) = 979\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 5\n",
      "circle 5 len(documents) = 899\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "Process _frequentPatternMining: n = 6\n",
      "circle 6 len(documents) = 688\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "Process _frequentPatternMining: n = 7\n",
      "circle 7 len(documents) = 444\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "Process _frequentPatternMining: n = 8\n",
      "circle 8 len(documents) = 274\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "Process _frequentPatternMining: n = 9\n",
      "circle 9 len(documents) = 161\n",
      "doc num 0\n",
      "doc num 100\n",
      "Process _frequentPatternMining: n = 10\n",
      "0\n",
      "монтаж 2829\n",
      "\n",
      "1\n",
      "душевой 72\n",
      "\n",
      "2\n",
      "кабинки 31\n",
      "\n",
      "3\n",
      "демонтаж 1060\n",
      "\n",
      "4\n",
      "счетчиков 39\n",
      "\n",
      "5\n",
      "воды 195\n",
      "\n",
      "6\n",
      "и 5904\n",
      "\n",
      "7\n",
      "тепла 27\n",
      "\n",
      "8\n",
      "прокладывание 23\n",
      "\n",
      "9\n",
      "труб 245\n",
      "\n",
      "10\n",
      "отопления 285\n",
      "\n",
      "len(doc_phrases) = 1000\n"
     ]
    }
   ],
   "source": [
    "import topmine_src\n",
    "import importlib # использую для перегрузки библиотек\n",
    "import topmine_src.phrase_mining as phrase_mining\n",
    "import sys\n",
    "import topmine_src.utils as utils\n",
    "\n",
    "importlib.reload(topmine_src.phrase_mining)\n",
    "importlib.reload(topmine_src.utils)\n",
    "\n",
    "arguments = sys.argv\n",
    "print('Running Phrase Mining...')\n",
    "\n",
    "file_name = \"docs_sample_1000\"\n",
    "output_path = \"DEBUG_remont_n-grams_2_1\"\n",
    "\n",
    "# represents the minimum number of occurences you want each phrase to have.\n",
    "min_support=2 \n",
    "\n",
    "# represents the threshold for merging two words into a phrase. A lower value\n",
    "# alpha leads to higher recall and lower precision,\n",
    "alpha=1\n",
    "\n",
    "# length of the maximum phrase size\n",
    "max_phrase_size=10\n",
    "\n",
    "phrase_miner = phrase_mining.PhraseMining(file_name, min_support, max_phrase_size, alpha);\n",
    "partitioned_docs, index_vocab = phrase_miner.mine()\n",
    "frequent_phrases = phrase_miner.get_frequent_phrases(min_support)\n",
    "frequent_hash_phrases = phrase_miner.get_frequent_phrases(min_support, counter_type=\"hash\")\n",
    "utils.store_partitioned_docs(partitioned_docs, output_path)\n",
    "utils.store_vocab(index_vocab, output_path)\n",
    "utils.store_frequent_phrases(frequent_phrases, output_path)\n",
    "utils.store_frequent_phrases(frequent_hash_phrases, output_path + \"_hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Phrase Mining...\n",
      "Start Mine10\n",
      "Start _frequentPatternMining\n",
      "_frequentPatternMining: len(documents) = 1000\n",
      "circle 2 len(documents) = 1000\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 3\n",
      "circle 3 len(documents) = 1000\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n",
      "doc num 500\n",
      "doc num 600\n",
      "doc num 700\n",
      "doc num 800\n",
      "doc num 900\n",
      "Process _frequentPatternMining: n = 4\n",
      "circle 4 len(documents) = 979\n",
      "doc num 0\n",
      "doc num 100\n",
      "doc num 200\n",
      "doc num 300\n",
      "doc num 400\n"
     ]
    }
   ],
   "source": [
    "import topmine_src\n",
    "import importlib # использую для перегрузки библиотек\n",
    "import topmine_src.phrase_mining as phrase_mining\n",
    "import sys\n",
    "import topmine_src.utils as utils\n",
    "\n",
    "importlib.reload(topmine_src.phrase_mining)\n",
    "importlib.reload(topmine_src.utils)\n",
    "\n",
    "arguments = sys.argv\n",
    "print('Running Phrase Mining...')\n",
    "\n",
    "file_name = \"docs_sample_1000\"\n",
    "output_path = \"DEBUG_remont_n-grams_2_10\"\n",
    "\n",
    "# represents the minimum number of occurences you want each phrase to have.\n",
    "min_support=2 \n",
    "\n",
    "# represents the threshold for merging two words into a phrase. A lower value\n",
    "# alpha leads to higher recall and lower precision,\n",
    "alpha=10\n",
    "\n",
    "# length of the maximum phrase size\n",
    "max_phrase_size=10\n",
    "\n",
    "phrase_miner = phrase_mining.PhraseMining(file_name, min_support, max_phrase_size, alpha);\n",
    "partitioned_docs, index_vocab = phrase_miner.mine()\n",
    "frequent_phrases = phrase_miner.get_frequent_phrases(min_support)\n",
    "frequent_hash_phrases = phrase_miner.get_frequent_phrases(min_support, counter_type=\"hash\")\n",
    "utils.store_partitioned_docs(partitioned_docs, output_path)\n",
    "utils.store_vocab(index_vocab, output_path)\n",
    "utils.store_frequent_phrases(frequent_phrases, output_path)\n",
    "utils.store_frequent_phrases(frequent_hash_phrases, output_path + \"_hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import topmine_src\n",
    "import importlib # использую для перегрузки библиотек\n",
    "import topmine_src.phrase_mining as phrase_mining\n",
    "import sys\n",
    "import topmine_src.utils as utils\n",
    "\n",
    "importlib.reload(topmine_src.phrase_mining)\n",
    "importlib.reload(topmine_src.utils)\n",
    "\n",
    "arguments = sys.argv\n",
    "print('Running Phrase Mining...')\n",
    "\n",
    "file_name = \"docs_sample_1000\"\n",
    "output_path = \"DEBUG_remont_n-grams_10_1000000000\"\n",
    "\n",
    "# represents the minimum number of occurences you want each phrase to have.\n",
    "min_support=10 \n",
    "\n",
    "# represents the threshold for merging two words into a phrase. A lower value\n",
    "# alpha leads to higher recall and lower precision,\n",
    "alpha=10**9\n",
    "\n",
    "# length of the maximum phrase size\n",
    "max_phrase_size=10\n",
    "\n",
    "phrase_miner = phrase_mining.PhraseMining(file_name, min_support, max_phrase_size, alpha);\n",
    "partitioned_docs, index_vocab = phrase_miner.mine()\n",
    "frequent_phrases = phrase_miner.get_frequent_phrases(min_support)\n",
    "frequent_hash_phrases = phrase_miner.get_frequent_phrases(min_support, counter_type=\"hash\")\n",
    "utils.store_partitioned_docs(partitioned_docs, output_path)\n",
    "utils.store_vocab(index_vocab, output_path)\n",
    "utils.store_frequent_phrases(frequent_phrases, output_path)\n",
    "utils.store_frequent_phrases(frequent_hash_phrases, output_path + \"_hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import topmine_src\n",
    "import importlib # использую для перегрузки библиотек\n",
    "import topmine_src.phrase_mining as phrase_mining\n",
    "import sys\n",
    "import topmine_src.utils as utils\n",
    "\n",
    "importlib.reload(topmine_src.phrase_mining)\n",
    "importlib.reload(topmine_src.utils)\n",
    "\n",
    "arguments = sys.argv\n",
    "print('Running Phrase Mining...')\n",
    "\n",
    "file_name = \"docs_sample_1000\"\n",
    "output_path = \"DEBUG_remont_n-grams_10_0_000000001\"\n",
    "\n",
    "# represents the minimum number of occurences you want each phrase to have.\n",
    "min_support=10 \n",
    "\n",
    "# represents the threshold for merging two words into a phrase. A lower value\n",
    "# alpha leads to higher recall and lower precision,\n",
    "alpha=0.1**9\n",
    "\n",
    "# length of the maximum phrase size\n",
    "max_phrase_size=10\n",
    "\n",
    "phrase_miner = phrase_mining.PhraseMining(file_name, min_support, max_phrase_size, alpha);\n",
    "partitioned_docs, index_vocab = phrase_miner.mine()\n",
    "frequent_phrases = phrase_miner.get_frequent_phrases(min_support)\n",
    "frequent_hash_phrases = phrase_miner.get_frequent_phrases(min_support, counter_type=\"hash\")\n",
    "utils.store_partitioned_docs(partitioned_docs, output_path)\n",
    "utils.store_vocab(index_vocab, output_path)\n",
    "utils.store_frequent_phrases(frequent_phrases, output_path)\n",
    "utils.store_frequent_phrases(frequent_hash_phrases, output_path + \"_hash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['А роза упала на лапу азора', ' АААА ФФФФ', '', '', ' ой-йой-йой']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"[.,;!?]\", \"А роза упала на лапу азора, АААА ФФФФ!!! ой-йой-йой\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_env",
   "language": "python",
   "name": "study_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
